{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "# Statistical Methods for Natural Language Semantics\n",
    "\n",
    "## Konstantin Todorov\n",
    "## Student number: 12402559\n",
    "\n",
    "### Repository link: https://github.com/ktodorov/uva-semantics-19\n",
    "\n",
    "\n",
    "<b>Results:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "from encoders.encoding_helper import EncodingHelper\n",
    "\n",
    "from helpers.cache_storage import CacheStorage\n",
    "from helpers.data_storage import DataStorage\n",
    "\n",
    "from inference_model import InferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sentence(sentence, w2i_dict, device):\n",
    "    sentence = word_tokenize(sentence)\n",
    "    indexes = torch.tensor([w2i_dict[word] for word in sentence]).to(device)\n",
    "    length = torch.Tensor([len(indexes)]).long().to(device)\n",
    "\n",
    "    return indexes, length\n",
    "\n",
    "\n",
    "def calculate_inference(model, token_vocabulary, label_dictionary, device, premise, hypothesis):\n",
    "    premise, premise_length = transform_sentence(\n",
    "        premise, token_vocabulary.stoi, device)\n",
    "\n",
    "    hypothesis, hypothesis_length = transform_sentence(\n",
    "        hypothesis, token_vocabulary.stoi, device)\n",
    "\n",
    "    inference_model = InferenceModel(\n",
    "        premise.expand(1, -1).transpose(0, 1),\n",
    "        premise_length,\n",
    "        hypothesis.expand(1, -1).transpose(0, 1),\n",
    "        hypothesis_length)\n",
    "\n",
    "    model_prediction = model.forward(inference_model)\n",
    "\n",
    "    print(\n",
    "        f\"The premise {label_dictionary[model_prediction.argmax().item()]} the hypothesis\")\n",
    "\n",
    "\n",
    "def initialize_data():\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    # Load the data sets and the vocabulary\n",
    "    print('Loading data...', end='')\n",
    "\n",
    "    data_storage = DataStorage()\n",
    "    token_vocabulary, _ = data_storage.get_vocabulary()\n",
    "\n",
    "    print('Loaded')\n",
    "\n",
    "    label_dictionary = {\n",
    "        0: \"entails\",\n",
    "        1: \"contradicts\",\n",
    "        2: \"is neutral to\"\n",
    "    }\n",
    "\n",
    "    return device, token_vocabulary, label_dictionary\n",
    "\n",
    "def initialize_model(model_path, device):\n",
    "    assert os.path.isfile(model_path), 'Model path is not valid'\n",
    "\n",
    "    # Check if we can get the cached model. If not, raise an exception\n",
    "    cache_storage = CacheStorage()\n",
    "    \n",
    "    print('Loading model...', end='')\n",
    "    # parameters_helper.snapshot_location)\n",
    "    model = cache_storage.load_model_snapshot(model_path)\n",
    "    if not model:\n",
    "        raise Exception('Model not found!')\n",
    "\n",
    "    print('Loaded')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...Loaded\n"
     ]
    }
   ],
   "source": [
    "device, token_vocabulary, label_dictionary = initialize_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...Loaded\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = 'results/mean/best_snapshot_devacc_60.797077922077925_devloss_0.9943482875823975__iter_25752_model.pt'\n",
    "# MODEL_PATH = 'results/uni-lstm/best_snapshot_devacc_34.52150974025974_devloss_1.0959851741790771__iter_25752_model.pt'\n",
    "\n",
    "model = initialize_model(MODEL_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The premise is neutral to the hypothesis\n"
     ]
    }
   ],
   "source": [
    "premise = 'the boy is doing a test'\n",
    "hypothesis = 'the boy is doing a test at the university'\n",
    "# hypothesis = 'the boy is doing a test well'\n",
    "\n",
    "calculate_inference(model, token_vocabulary,\n",
    "                    label_dictionary, device, premise, hypothesis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
